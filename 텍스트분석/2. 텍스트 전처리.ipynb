{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-supervision",
   "metadata": {},
   "source": [
    "## Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-course",
   "metadata": {},
   "source": [
    "### 1) 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "living-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Miso\n",
      "[nltk_data]     CHOI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Miso\n",
      "[nltk_data]     CHOI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Miso\n",
      "[nltk_data]     CHOI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrong-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "['처음부터 Google은 최고의 사용자 환경을 제공하는 데 초점을 맞췄습니다.', 'Google은 새로운 인터넷 브라우저를 개발하든 홈페이지의 디자인을 새롭게 변경하든 언제나 내부의 목표나 수익보다는 사용자에게 최상의 서비스를 제공하는 것을 우선 고려합니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = '처음부터 Google은 최고의 사용자 환경을 제공하는 데 초점을 맞췄습니다. \\\n",
    "Google은 새로운 인터넷 브라우저를 개발하든 홈페이지의 디자인을 새롭게 \\\n",
    "변경하든 언제나 내부의 목표나 수익보다는 사용자에게 최상의 서비스를 \\\n",
    "제공하는 것을 우선 고려합니다.'\n",
    "\n",
    "# 문장 자르는 함수 : sent_tokenize\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-retrieval",
   "metadata": {},
   "source": [
    "### 2) 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grand-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 10\n",
      "['처음부터', 'Google은', '최고의', '사용자', '환경을', '제공하는', '데', '초점을', '맞췄습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = '처음부터 Google은 최고의 사용자 환경을 제공하는 데 초점을 맞췄습니다.'\n",
    "\n",
    "# 공백 기준으로 단어 단위로 구분하는 함수 : word_tokenize\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words) # 마침표, 쉼표도 잘림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-durham",
   "metadata": {},
   "source": [
    "### 3) 여러 문장들에 대한 단어 토큰화 (함수 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finnish-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "[['처음부터', 'Google은', '최고의', '사용자', '환경을', '제공하는', '데', '초점을', '맞췄습니다', '.'], ['Google은', '새로운', '인터넷', '브라우저를', '개발하든', '홈페이지의', '디자인을', '새롭게', '변경하든', '언제나', '내부의', '목표나', '수익보다는', '사용자에게', '최상의', '서비스를', '제공하는', '것을', '우선', '고려합니다', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# 함수 생성: 여러 문장 -> 문장별 토큰화된 단어 \n",
    "def tokenize_text(text):\n",
    "    # 문장별로 분리\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "    \n",
    "    \n",
    "text_sample = '처음부터 Google은 최고의 사용자 환경을 제공하는 데 초점을 맞췄습니다. \\\n",
    "Google은 새로운 인터넷 브라우저를 개발하든 홈페이지의 디자인을 새롭게 \\\n",
    "변경하든 언제나 내부의 목표나 수익보다는 사용자에게 최상의 서비스를 \\\n",
    "제공하는 것을 우선 고려합니다.'\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-cause",
   "metadata": {},
   "source": [
    "## n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "suspended-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('처음부터', 'Google은', '최고의'), ('Google은', '최고의', '사용자'), ('최고의', '사용자', '환경을'), ('사용자', '환경을', '제공하는'), ('환경을', '제공하는', '데'), ('제공하는', '데', '초점을'), ('데', '초점을', '맞췄습니다'), ('초점을', '맞췄습니다', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = '처음부터 Google은 최고의 사용자 환경을 제공하는 데 초점을 맞췄습니다.'\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "all_grams = ngrams(words,3)\n",
    "n_grams = [ngram for ngram in all_grams]\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-coupon",
   "metadata": {},
   "source": [
    "### Stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "convenient-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['처음부터', 'Google은', '최고의', '사용자', '환경을', '제공하는', '데', '초점을', '맞췄습니다', '.'], ['Google은', '새로운', '인터넷', '브라우저를', '개발하든', '홈페이지의', '디자인을', '새롭게', '변경하든', '언제나', '내부의', '목표나', '수익보다는', '사용자에게', '최상의', '서비스를', '제공하는', '것을', '고려합니다', '.']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 영어의 경우, nltk에서 stopwords 제공 (179개)\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# 한국어는 제공X -> 조사, 접속사, 전치사, 어미, 감탄사 등을 담은 텍스트 파일 생성함\n",
    "df_stop = pd.read_table('./stopwords_korean.txt', header = None, sep = \"\\n\")\n",
    "list_stop_kr = list(pd.Series(df_stop[0]))\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        if word not in list_stop_kr:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-daniel",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "advance-batman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer() # 객체 생성\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aerial-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "# 품사를 찾아서 넣어줘야 함 (v: 동사, a: 형용사)\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))\n",
    "# Lemmatization이 Stemming보다 정교하고 의미론적 기반에서 단어 원형을 찾아줌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
